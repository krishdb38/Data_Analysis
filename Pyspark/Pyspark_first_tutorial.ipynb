{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SparkSession\n",
    "# If java not installed properly Error will raise \n",
    "# Uninstall java and re install\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"Python spark First example\")\\\n",
    ".config(\"spark.some.config.option\", \"some-value\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sparkContext.parallelize([(1,2,3,\"a b c\"),\n",
    "                                     (4,5,6,\"d e f\"),\n",
    "                                     (7,8,9,\"g h i\")\n",
    "                                    ])\\\n",
    "                                    .toDF([\"col1\" , \"col2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+-----+\n",
      "|col1|col2| _3|   _4|\n",
      "+----+----+---+-----+\n",
      "|   1|   2|  3|a b c|\n",
      "|   4|   5|  6|d e f|\n",
      "|   7|   8|  9|g h i|\n",
      "+----+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark create RDD example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "df = spark.sparkContext\\\n",
    ".parallelize([(1, 2, 3, 'a b c'),\n",
    "(4, 5, 6, 'd e f'),\n",
    "(7, 8, 9, 'g h i')])\\\n",
    ".toDF(['col1', 'col2', 'col3','col4'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python first program to create RDD\")\\\n",
    "        .config(\"spark.some.config.option\",\"some-value\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employee = spark.createDataFrame([\n",
    "    (1,\"Joe\", \"10000\",1),\n",
    "    (2,\"Joe\", \"10000\",1),\n",
    "    (3,\"Joe\", \"10000\",1),\n",
    "    (4,\"Joe\", \"10000\",1),\n",
    "    (5,\"Joe\", \"10000\",1),\n",
    "],[\"ID\",\"Name\",\"Sallary\",\"Department\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+----------+\n",
      "| ID|Name|Sallary|Department|\n",
      "+---+----+-------+----------+\n",
      "|  1| Joe|  10000|         1|\n",
      "|  2| Joe|  10000|         1|\n",
      "|  3| Joe|  10000|         1|\n",
      "|  4| Joe|  10000|         1|\n",
      "|  5| Joe|  10000|         1|\n",
      "+---+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sallary</th>\n",
       "      <th>Department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Joe</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Joe</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Joe</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Joe</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Joe</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Name Sallary  Department\n",
       "0   1  Joe   10000           1\n",
       "1   2  Joe   10000           1\n",
       "2   3  Joe   10000           1\n",
       "3   4  Joe   10000           1\n",
       "4   5  Joe   10000           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Employee.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. By Using Read  or load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark create RDD example\")\\\n",
    "        .config(\"spark.some.config.option\" , \"some-value\").getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").\\\n",
    "                        options(header = \"true\",\\\n",
    "                               inferschema = \"true\").\\\n",
    "                    load(\"../data/super_store.csv\", header = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------+---------+--------------+-----------+---------------+-----------+-----------+---------------+---------+-----------+------+-------+----------------+---------------+------------+--------------------+-------+--------+--------+-------+-------------+--------------+\n",
      "|Row ID|       Order ID|Order Date|Ship Date|     Ship Mode|Customer ID|  Customer Name|    Segment|       City|          State|  Country|Postal Code|Market| Region|      Product ID|       Category|Sub-Category|        Product Name|  Sales|Quantity|Discount| Profit|Shipping Cost|Order Priority|\n",
      "+------+---------------+----------+---------+--------------+-----------+---------------+-----------+-----------+---------------+---------+-----------+------+-------+----------------+---------------+------------+--------------------+-------+--------+--------+-------+-------------+--------------+\n",
      "| 42433|   AG-2011-2040|  1/1/2011| 6/1/2011|Standard Class|   TB-11280|Toby Braunhardt|   Consumer|Constantine|    Constantine|  Algeria|       null|Africa| Africa|OFF-TEN-10000025|Office Supplies|     Storage| Tenex Lockers, Blue|  408.3|       2|       0| 106.14|        35.46|        Medium|\n",
      "| 22253|  IN-2011-47883|  1/1/2011| 8/1/2011|Standard Class|   JH-15985|    Joseph Holt|   Consumer|Wagga Wagga|New South Wales|Australia|       null|  APAC|Oceania| OFF-SU-10000618|Office Supplies|    Supplies|Acme Trimmer, Hig...|120.366|       3|     0.1| 36.036|         9.72|        Medium|\n",
      "| 48883|   HU-2011-1220|  1/1/2011| 5/1/2011|  Second Class|     AT-735|  Annie Thurman|   Consumer|   Budapest|       Budapest|  Hungary|       null|  EMEA|   EMEA|OFF-TEN-10001585|Office Supplies|     Storage|Tenex Box, Single...|  66.12|       4|       0|  29.64|         8.17|          High|\n",
      "| 11731|IT-2011-3647632|  1/1/2011| 5/1/2011|  Second Class|   EM-14140|   Eugene Moren|Home Office|  Stockholm|      Stockholm|   Sweden|       null|    EU|  North| OFF-PA-10001492|Office Supplies|       Paper|Enermax Note Card...| 44.865|       3|     0.5|-26.055|         4.82|          High|\n",
      "| 22255|  IN-2011-47883|  1/1/2011| 8/1/2011|Standard Class|   JH-15985|    Joseph Holt|   Consumer|Wagga Wagga|New South Wales|Australia|       null|  APAC|Oceania| FUR-FU-10003447|      Furniture| Furnishings|Eldon Light Bulb,...| 113.67|       5|     0.1|  37.77|          4.7|        Medium|\n",
      "+------+---------------+----------+---------+--------------+-----------+---------------+-----------+-----------+---------------+---------+-----------+------+-------+----------------+---------------+------------+--------------------+-------+--------+--------+-------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Postal Code: integer (nullable = true)\n",
      " |-- Market: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      " |-- Shipping Cost: double (nullable = true)\n",
      " |-- Order Priority: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # printout the schema out in the Tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once created, RDDs offer two types of operations: transformations and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset from DataBase\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark create RDD example\")\\\n",
    "        .config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"root\"\n",
    "pw = \"ubuntu\"\n",
    "table_name = \"city\"\n",
    "url = \"localhost@root.com\"\n",
    "# see page 38 pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset from HDFS # page 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
